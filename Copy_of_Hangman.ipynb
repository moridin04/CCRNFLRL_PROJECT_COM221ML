{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moridin04/CCRNFLRL_PROJECT_COM221ML/blob/main/Copy_of_Hangman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DITVQsd1yj2u"
      },
      "source": [
        "# **Training Deep Q-Network Agent to Play Hangman**\n",
        "\n",
        "Roadmap\n",
        "1. Markov Decision Process (MDP)\n",
        "- State Space\n",
        "- Action Space - Discrete(18)\n",
        "0 - NOOP\n",
        "\n",
        "1 - FIRE\n",
        "\n",
        "2 - UP\n",
        "\n",
        "3 - RIGHT\n",
        "\n",
        "4 - LEFT\n",
        "\n",
        "5 - DOWN\n",
        "\n",
        "6 - UPRIGHT\n",
        "\n",
        "7 - UPLEFT\n",
        "\n",
        "8 - DOWNRIGHT\n",
        "\n",
        "9 - DOWNLEFT\n",
        "\n",
        "10 - UPFIRE\n",
        "\n",
        "11 - RIGHTFIRE\n",
        "\n",
        "12 - LEFTFIRE\n",
        "\n",
        "13 - DOWNFIRE\n",
        "\n",
        "14 - UPRIGHTFIRE\n",
        "\n",
        "15 - UPLEFTFIRE\n",
        "\n",
        "16 - DOWNRIGHTFIRE\n",
        "\n",
        "17 - DOWNLEFTFIRE\n",
        "\n",
        "\n",
        "- Rewards\n",
        "\n",
        "2. Setting up \"Hangman\" environment (Gymnasium)\n",
        "- Observation Encoding\n",
        "- Action Encoding\n",
        "- Reward Shaping Strategy\n",
        "\n",
        "3. Deep Q-Network\n",
        "- Input Layer (Word State Encoding)\n",
        "- Hidden Layer (Fully Connected Layers / Embeddings)\n",
        "- Output Layer (Q-Values of Each Letter)\n",
        "- Training Algorithm (Experience Replay, Target Network)\n",
        "- Hyperparameters (Learning Rate, y, Epsilon-Greedy, Replay Buffer Size, Batch Size)\n",
        "\n",
        "4. Training Process\n",
        "- Epsilon Decay\n",
        "- Replay Buffer (filling and sampling)\n",
        "- Target Network Update Frequency\n",
        "- Number of Episodes and Training Duration\n",
        "\n",
        "5. Evaluation Metrics\n",
        "- Win Rate\n",
        "- Average Reward per Episode\n",
        "- Average Number of Steps until Win/Lose\n",
        "\n",
        "6. Record Multiple Episodes as Video\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8FGXWMci5OD"
      },
      "source": [
        "## **Installing and Importing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mu1TAT_yEiN",
        "outputId": "d40c3ac1-8ef1-4c4a-d663-bde4c80be0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[atari]) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --pre \"gymnasium[atari]\" ale-py\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym #Hangman Environment\n",
        "import ale_py\n",
        "import torch #Neural Networks\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque #Replay Buffer\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from IPython.display import Video\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "from skimage import transform\n",
        "import tensorflow as tf\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exe8AvKSi_h4"
      },
      "source": [
        "## **Check Device Name**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOlxwWRMEwqE",
        "outputId": "848c89bd-64e1-40a8-b372-345cd4cb3a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzS6T0_zjL3S"
      },
      "source": [
        "## **Trying Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SpEC_eCi3yVY"
      },
      "outputs": [],
      "source": [
        "def create_environment():\n",
        "    env = gym.make(\"ALE/Hangman-v5\", render_mode=\"rgb_array\")\n",
        "\n",
        "    noop = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    fire = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    up = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    right = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    left = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    down = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    upright = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    upleft = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    downright = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    downleft = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    upfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "    rightfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "    leftfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
        "    downfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    uprightfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
        "    upleftfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
        "    downrightfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "    downleftfire = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
        "    possible_actions = [noop, fire, up, right, left, down, upright, upleft, downright, downleft, upfire, rightfire, leftfire, downfire, uprightfire, upleftfire, downrightfire, downleftfire]\n",
        "\n",
        "    return env, possible_actions\n",
        "\n",
        "def test_environment():\n",
        "    env = gym.make(\"ALE/Hangman-v5\", render_mode=\"rgb_array\")\n",
        "\n",
        "    episodes = 3\n",
        "    for episode in range(episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            print(f\"Reward: {reward}\")\n",
        "            time.sleep(0.02)\n",
        "\n",
        "        print(f\"Episode: {episode+1}, Total Reward: {total_reward}\\n\")\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "metadata": {
        "id": "2VvvoqJs0WJ5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDBw5t-NjHY4"
      },
      "source": [
        "## **Pre-processing (Converting to grayscale and resize)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "    # Convert to grayscale if RGB\n",
        "    if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
        "        frame = rgb2gray(frame)\n",
        "\n",
        "    # Crop and normalize\n",
        "    cropped_frame = frame[34:34+192, :160]\n",
        "    normalized_frame = cropped_frame / 255.0\n",
        "\n",
        "    # Resize to 84x84\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\n",
        "\n",
        "    return preprocessed_frame"
      ],
      "metadata": {
        "id": "zNUIUWXz2Clf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jj6iho8jUZC"
      },
      "source": [
        "## **Stacking Frames**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tQkMrccEKyQ4"
      },
      "outputs": [],
      "source": [
        "stack_size = 4\n",
        "stacked_frames = deque([np.zeros((84,84), dtype=np.int64) for _ in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    frame = preprocess_frame(state)\n",
        "\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int64) for _ in range(stack_size)], maxlen=4)\n",
        "        for _ in range(4):\n",
        "          stacked_frames.append(frame)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "    stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    return stacked_state, stacked_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameters**"
      ],
      "metadata": {
        "id": "-kqGB7yc3prI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = [84,84,4]\n",
        "action_size = len(possible_actions)\n",
        "learning_rate = 0.0005\n",
        "\n",
        "total_episodes = 1000\n",
        "batch_size = 128\n",
        "\n",
        "explore_start = 1.0\n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.0001\n",
        "\n",
        "gamma = 0.99\n",
        "memory_size = 1000000"
      ],
      "metadata": {
        "id": "DV6GWGKy3yt_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSBY8K13jOsj"
      },
      "source": [
        "## **Creating Deep Q-Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5jKV21PF4eGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861c8f04-1080-4f09-8049-e2b7bbfd9590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        with tf.compat.v1.variable_scope(name):\n",
        "            self.inputs_ = tf.compat.v1.placeholder(tf.float32, [None, *state_size], name='inputs')\n",
        "            self.actions_ = tf.compat.v1.placeholder(tf.float32, [None, action_size], name='actions_')\n",
        "            self.target_Q = tf.compat.v1.placeholder(tf.float32, [None], name='target')\n",
        "\n",
        "            self.conv1 = tf.keras.layers.Conv2D(32, [8,8], [4,4], activation='elu')(self.inputs_)\n",
        "            self.conv2 = tf.keras.layers.Conv2D(64, [4,4], [2,2], activation='elu')(self.conv1)\n",
        "            self.conv3 = tf.keras.layers.Conv2D(128, [4,4], [2,2], activation='elu')(self.conv2)\n",
        "\n",
        "            self.flatten = tf.keras.layers.Flatten()(self.conv3)\n",
        "            self.fc = tf.keras.layers.Dense(512, activation='elu')(self.flatten)\n",
        "            self.output = tf.keras.layers.Dense(action_size)(self.fc)\n",
        "\n",
        "\n",
        "            # Safe multiply with guaranteed matching shapes\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
        "\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            self.optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv2zQ4ehjSbu"
      },
      "source": [
        "## **Replay Buffer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u_gOElt587DG"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size), size = batch_size, replace = False)\n",
        "\n",
        "        return [self.buffer[i] for i in index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = Memory(max_size = memory_size)\n",
        "state, _ = game.reset()\n",
        "\n",
        "for i in range(batch_size):\n",
        "    if i == 0:\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    action_index = random.randrange(action_size)\n",
        "    next_state, reward, terminated, truncated, _ = game.step(action_index)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done:\n",
        "        next_state = np.zeros(state.shape)\n",
        "        memory.add((state, action_index, reward, next_state, done))\n",
        "        state, _ = game.reset()\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    else:\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        memory.add((state, action_index, reward, next_state, done))\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "SXX8nXy6Th4a"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3zqsFHjXBX"
      },
      "source": [
        "## **Training Deep Q-Network Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aLVwoZIn9F99"
      },
      "outputs": [],
      "source": [
        "def predict_action(sess, explore_start, explore_stop, decay_rate, decay_step, state, possible_actions):\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "\n",
        "    if explore_probability > exp_exp_tradeoff:\n",
        "        action_index = random.randrange(len(possible_actions))\n",
        "    else:\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "        action_index = np.argmax(Qs)\n",
        "    return action_index, explore_probability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(env, total_episodes, max_steps, memory, DQNetwork,\n",
        "              explore_start, explore_stop, decay_rate, gamma, batch_size):\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "    episode_rewards_list = []\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        decay_step = 0\n",
        "\n",
        "        for episode in range(total_episodes):\n",
        "            obs, info = env.reset()\n",
        "            state, stacked_frames = stack_frames(stacked_frames=None, state=obs, is_new_episode=True)\n",
        "            done = False\n",
        "            episode_rewards = []\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                decay_step += 1\n",
        "                action_index, explore_probability = predict_action(\n",
        "                    sess, explore_start, explore_stop, decay_rate, decay_step, state, possible_actions\n",
        "                )\n",
        "\n",
        "                next_obs, reward, terminated, truncated, info = env.step(action_index)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "                memory.add((state, action_index, reward, next_state, done))\n",
        "                state = next_state\n",
        "                episode_rewards.append(reward)\n",
        "\n",
        "                if done:\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "                    print(f\"Episode {episode+1}/{total_episodes} | Total Reward: {total_reward:.2f} | Explore P: {explore_probability:.4f}\")\n",
        "                    break\n",
        "\n",
        "                # --- Learning step ---\n",
        "                if len(memory.buffer) > batch_size:\n",
        "                    batch = memory.sample(batch_size)\n",
        "                    states_mb = np.array([each[0] for each in batch])\n",
        "                    actions_mb = np.array([each[1] for each in batch])\n",
        "                    rewards_mb = np.array([each[2] for each in batch])\n",
        "                    next_states_mb = np.array([each[3] for each in batch])\n",
        "                    dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                    target_Qs_batch = []\n",
        "                    Qs_next_state = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: next_states_mb})\n",
        "\n",
        "                    for i in range(batch_size):\n",
        "                        if dones_mb[i]:\n",
        "                            target_Qs_batch.append(rewards_mb[i])\n",
        "                        else:\n",
        "                            target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                            target_Qs_batch.append(target)\n",
        "\n",
        "                    targets_mb = np.array(target_Qs_batch)\n",
        "\n",
        "                    actions_one_hot = np.eye(len(possible_actions))[actions_mb]\n",
        "                    loss, _ = sess.run(\n",
        "                        [DQNetwork.loss, DQNetwork.optimizer],\n",
        "                        feed_dict={\n",
        "                            DQNetwork.inputs_: states_mb,\n",
        "                            DQNetwork.target_Q: targets_mb,\n",
        "                            DQNetwork.actions_: actions_one_hot,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            total_reward = np.sum(episode_rewards)\n",
        "            episode_rewards_list.append(total_reward)\n",
        "\n",
        "            if episode % 5 == 0:\n",
        "                saver.save(sess, \"./models/hangman_dqn.ckpt\")\n",
        "                print(\"Model saved.\")\n",
        "\n",
        "        env.close()\n",
        "        return episode_rewards_list"
      ],
      "metadata": {
        "id": "7C-cTC60fL-b"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_progress(rewards, window=20):\n",
        "    if rewards is None or len(rewards) == 0:\n",
        "        print(\"No rewards to plot yet.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Raw rewards\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards, color=\"blue\", linewidth=1.5)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"DQN Training on Hangman-v5\")\n",
        "\n",
        "    # Moving average\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if len(rewards) > window:\n",
        "        moving_avg = [np.mean(rewards[i:i+window]) for i in range(len(rewards)-window)]\n",
        "        plt.plot(moving_avg, color=\"orange\", linewidth=2)\n",
        "        plt.title(f\"Moving Average Reward (window={window})\")\n",
        "    else:\n",
        "        plt.plot(rewards, color=\"orange\", linewidth=2)\n",
        "        plt.title(\"Moving Average Reward (too few episodes)\")\n",
        "\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DIv9KUafjRxg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3kLkBzzBcbf",
        "outputId": "c00b4bfe-7f64-456d-dfa2-f70cb7da4d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "rewards = train_dqn(\n",
        "    env=game,\n",
        "    total_episodes=1000,\n",
        "    max_steps=100,\n",
        "    memory=memory,\n",
        "    DQNetwork=DQNetwork,\n",
        "    explore_start=explore_start,\n",
        "    explore_stop=explore_stop,\n",
        "    decay_rate=decay_rate,\n",
        "    gamma=gamma,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lp6UMwmhGq"
      },
      "source": [
        "## **Plotting DQN Training per Episode and Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_progress(rewards, window=50)"
      ],
      "metadata": {
        "id": "OHjF79-Hk0zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ndO-1kmSmu"
      },
      "source": [
        "## **Recording all Episodes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FNMrjA6BmcI"
      },
      "outputs": [],
      "source": [
        "def record_episode(env, DQNetwork, possible_actions, stacked_frames, filename=\"hangman_episode.mp4\"):\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        # Load the trained model\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.restore(sess, \"./models/hangman_dqn.ckpt\")\n",
        "\n",
        "        obs, info = env.reset()\n",
        "        state, stacked_frames = stack_frames(None, obs, True)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        frames = []\n",
        "\n",
        "        while not done:\n",
        "            # Get Q-values from the trained network\n",
        "            Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "            choice = np.argmax(Qs)\n",
        "            action_index = int(choice)\n",
        "\n",
        "            # Step in environment\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action_index)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "            # Preprocess and stack frames\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "            state = next_state\n",
        "\n",
        "            # Render frame\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.close()\n",
        "        print(f\"Total score (reward): {total_reward}\")\n",
        "\n",
        "        # Save video\n",
        "        imageio.mimsave(filename, frames, fps=15)\n",
        "        print(f\"Episode saved to {filename}\")\n",
        "\n",
        "        from IPython.display import Video\n",
        "        return Video(filename, embed=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env, possible_actions = create_environment()\n",
        "\n",
        "for i in range(10):  # record 10 episodes\n",
        "    filename = f\"trained_hangman_ep{i+1}.mp4\"\n",
        "    print(f\"\\n🎥 Recording Episode {i+1}/10...\")\n",
        "    record_episode(env, DQNetwork, possible_actions, stacked_frames, filename=filename)\n"
      ],
      "metadata": {
        "id": "0iWhHaWJv_JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "clips = [VideoFileClip(f\"trained_hangman_ep{i+1}.mp4\") for i in range(10)]\n",
        "final_clip = concatenate_videoclips(clips)\n",
        "final_clip.write_videofile(\"hangman_all_episodes.mp4\", codec=\"libx264\", fps=15)\n",
        "\n",
        "print(\"Combined video saved as hangman_all_episodes.mp4\")\n"
      ],
      "metadata": {
        "id": "MFT-LLZsqQZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video(\"hangman_all_episodes.mp4\", embed=True)"
      ],
      "metadata": {
        "id": "qsZ1NsZxqXSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOhAyZyXkP5A9JMAg8OWx2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}